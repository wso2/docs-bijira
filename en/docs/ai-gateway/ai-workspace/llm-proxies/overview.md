# LLM Proxies Overview

LLM Proxies provide a managed API gateway layer that sits between your applications and LLM providers. They enable you to create standardized endpoints with built-in security, monitoring, and governance capabilities while maintaining full control over how your applications interact with Large Language Models.

By configuring proxies in the AI Workspace, you can:

- **Unified interface**: Create consistent API endpoints regardless of the underlying provider
- **Policy enforcement**: Apply rate limiting, guardrails, and security policies
- **Traffic management**: Monitor and control API usage
- **Cost optimization**: Implement caching and smart routing to reduce costs
- **Security**: Add authentication and authorization layers

[//]: # (## Quick Start)

[//]: # ()
[//]: # (To start using LLM Proxies:)

[//]: # ()
[//]: # (1. Navigate to AI Workspace in your Bijira dashboard)

[//]: # (2. Select "LLM Proxies" from the menu)

[//]: # (3. Click "Create Proxy" and fill in the basic details)

[//]: # (4. Configure your resources, security, and guardrails)

[//]: # (5. Save and deploy your proxy)

**Next:** [Configure LLM Proxy](create-proxy.md) - Step-by-step guide to configure and deploy your first proxy
